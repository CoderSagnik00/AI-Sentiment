{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e06a02-ee98-40e5-a851-dcb208b40eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e6493b-4be7-4df5-bfb0-1d40482bf169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        # Disable oneDNN optimizations warning\n",
    "        os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "        # Model and Tokenizer Name\n",
    "        model_name = \"google-bert/bert-base-uncased\"  # Replace with a valid model name\n",
    "\n",
    "        # Load Tokenizer and Model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=3\n",
    "        )\n",
    "\n",
    "    def SetDatasets(self, texts, labels):\n",
    "        # Example Dataset (Texts and Labels)\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "        self.train_data, self.train_labels = self.preprocess(self.texts, self.labels)\n",
    "\n",
    "    def SplitDataset(self, testDataPercentage):\n",
    "        pass\n",
    "\n",
    "    def LoadModelNTokenizer(self, modelLocation=\"./trained\"):\n",
    "        # Load the model and tokenizer from the saved directory\n",
    "        loaded_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            modelLocation\n",
    "        )\n",
    "        loaded_tokenizer = AutoTokenizer.from_pretrained(modelLocation)\n",
    "        return loaded_model, loaded_tokenizer\n",
    "\n",
    "    def SaveModelNTokenizer(self, modelLocation=\"./trained\"):\n",
    "        # Save the Model and Tokenizer\n",
    "        self.model.save_pretrained(modelLocation)\n",
    "        self.tokenizer.save_pretrained(modelLocation)\n",
    "\n",
    "    # Tokenize the Dataset\n",
    "    def preprocess(self, texts, labels, max_length=128):\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "        return encodings, tf.convert_to_tensor(labels)\n",
    "\n",
    "    def CompileModel(self):\n",
    "        # Compile the Model (use modern loss function to avoid deprecation)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    def TrainModel(self, batchSize=2, epochs=3):\n",
    "        self.CompileModel()\n",
    "        # Train the Model\n",
    "        self.history = self.model.fit(\n",
    "            self.train_data, self.train_labels, batch_size=batchSize, epochs=epochs\n",
    "        )\n",
    "    def plot_training_history(self):\n",
    "        # Check if training history exists\n",
    "        if not hasattr(self, 'history'):\n",
    "            print(\"Model hasn't been trained yet!\")\n",
    "            return\n",
    "        \n",
    "        # Get history data\n",
    "        history = self.history.history\n",
    "        \n",
    "        # Plot Training & Validation Accuracy\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Accuracy Plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['sparse_categorical_accuracy'], label='Train Accuracy')\n",
    "        plt.title('Train Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['loss'], label='Train Loss')\n",
    "        plt.title('Train Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd139a68-4829-4d67-9709-1bc51b02d2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI-Sentiment\\model\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\AI-Sentiment\\model\\env\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1b6ea862-2274-49e5-b767-9e72667c16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"This is amazing!\", \"I hate it.\"]\n",
    "test_data, _ = model.preprocess(test_texts, [1, 0])\n",
    "predictions = loaded_model.predict(test_data)\n",
    "# Process and Print Predictions\n",
    "predicted_classes = np.argmax(predictions.logits, axis=1)\n",
    "print(f\"Predictions: {predicted_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8a2415-f652-461d-9ea5-d26cd4f9a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for loading datasets\n",
    "csv_file = \"./datasets/cleaned_split-data.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "data = Dataset.from_pandas(df) \n",
    "\n",
    "sentiments = list()\n",
    "comments = data['Comments']\n",
    "for i in data['Sentiment']:\n",
    "    sen = i.lower()\n",
    "    if sen == \"positive\":\n",
    "        sentiments.append(1)\n",
    "    elif sen == \"negative\":\n",
    "        sentiments.append(0)\n",
    "    elif sen == \"neutral\":\n",
    "        sentiments.append(2)\n",
    "\n",
    "\n",
    "comments_train, comments_test, sentiments_train, sentiments_test = train_test_split(comments, sentiments, test_size=0.2, random_state=42)\n",
    "\n",
    "model.SetDatasets(texts=comments_train, labels=sentiments_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570acad8-df5d-4e33-95cf-8baf2238c6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model.TrainModel(epochs=5, batchSize=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965a5612-2037-453f-9fc1-e3d21aca3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.SaveModelNTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2e58dbd-a305-49c4-ae04-89d5054d0acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hasn't been trained yet!\n"
     ]
    }
   ],
   "source": [
    "model.plot_training_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc2139-f3b4-4bd4-9ad9-020609398b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
